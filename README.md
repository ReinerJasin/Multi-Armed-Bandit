# Multi-Armed-Bandit
Implementation of the Multi-Armed Bandit where each arm returns continuous numerical rewards. Covers Epsilon-Greedy, UCB1, and Thompson Sampling with detailed explanations.
