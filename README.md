# Multi-Armed-Bandit
Implementation of the Multi-Armed Bandit where each arm returns continuous numerical rewards. Covers Epsilon-Greedy, UCB1, and Thompson Sampling with detailed explanations.

# Multi-Armed Bandit Implementation on Numerical Data
This repository explores the Multi-Armed Bandit problem using numerical data instead of the traditional Bernoulli distribution (which returns only 0 or 1). It provides a comprehensive overview of the fundamental concepts, alongside practical implementations of three popular algorithms:

âœ… Epsilon-Greedy â€“ Balancing exploration and exploitation through a probability-based approach.
âœ… UCB1 (Upper Confidence Bound) â€“ Optimizing decision-making with confidence intervals.
âœ… Thompson Sampling â€“ A Bayesian approach to adaptive learning.

The notebook includes detailed explanations, code implementations, and visualizations to help you understand how these algorithms work in real-world scenarios.

ðŸ“Œ Ideal for: Data scientists, AI researchers, and anyone interested in reinforcement learning.

Feel free to explore, experiment, and contribute! ðŸš€
